
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{NewtonCode}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Newton's Method for Logistic Regression - The Math of
Intelligence (Week
2)}\label{newtons-method-for-logistic-regression---the-math-of-intelligence-week-2}

\begin{figure}
\centering
\includegraphics{https://plot.ly/~florianh/140/logistic-regression-1-feature.png}
\caption{alt text}
\end{figure}

\subsection{Our Task}\label{our-task}

We're going to compute the probability that someone has Diabetes given
their height, weight, and blood pressure. We'll generate this data
ourselves (toy data), plot it, learn a logistic regression curve using
Newton's Method for Optimization, then use that curve to predict the
probability someone new with these 3 features has diabetes. We'll use
Calculus, Probability Theory, Statistics, and Linear Algebra to do this.
Get ready, ish is about to go down.

\subsection{What is Logistic
regression?}\label{what-is-logistic-regression}

Logistic regression is named for the function used at the core of the
method, the logistic function. In linear regression, the outcome
(dependent variable) is continuous. It can have any one of an infinite
number of possible values. In logistic regression, the outcome
(dependent variable) has only a limited number of possible values.
Logistic Regression is used when response variable is categorical in
nature.

The logistic function, also called the sigmoid function is an S-shaped
curve that can take any real-valued number and map it into a value
between 0 and 1, but never exactly at those limits.

\begin{figure}
\centering
\includegraphics{https://qph.ec.quoracdn.net/main-qimg-05edc1873d0103e36064862a45566dba}
\caption{alt text}
\end{figure}

Where e is the base of the natural logarithms (Euler's number or the
EXP() function in your spreadsheet) and value is the actual numerical
value that you want to transform. E is a really convenient number for
math, for example Whenever you take the derivative of e\^{}x (that's e
to the x), you get e\^{}x back again. It's the only function on Earth
that will do that.

Logistic regression uses an equation as the representation, similar to
linear regression. The central premise of Logistic Regression is the
assumption that your input space can be separated into two nice
`regions', one for each class, by a linear(read: straight) boundary.
Your data must be linearly seperable in n dimensions

\begin{figure}
\centering
\includegraphics{https://codesachin.files.wordpress.com/2015/08/linearly_separable_4.png}
\caption{alt text}
\end{figure}

So if we had the following function

\begin{figure}
\centering
\includegraphics{https://s0.wp.com/latex.php?latex=\%5Cbeta_0+\%2B+\%5Cbeta_1+x_1+\%2B+\%5Cbeta_2+x_2\&bg=ffffff\&fg=555555\&s=0\&zoom=2}
\caption{alt text}
\end{figure}

Given some point (a,b), if we plugged it in, the equation could output a
positive result (for one class), negative result (for the other class),
or 0 (the point lies right on the decision boundary).

So we have a function that outputs a value in (-infinity, +infinity)
given an input data point. But how do we map this to the probability
P\_+, that goes from {[}0, 1{]}? The answer, is in the odds function.

\begin{figure}
\centering
\includegraphics{http://i.imgur.com/bz8XqI8.png}
\caption{alt text}
\end{figure}

Let \(P(X)\) denote the probability of an event X occurring. In that
case, the odds ratio (OR(X)) is defined by

\begin{figure}
\centering
\includegraphics{https://s0.wp.com/latex.php?latex=\%5Cfrac\%7BP\%28X\%29\%7D\%7B1-P\%28X\%29\%7D\&bg=ffffff\&fg=555555\&s=0\&zoom=2}
\caption{alt text}
\end{figure}

which is essentially the ratio of the probability of the event
happening, vs. it not happening. It is clear that probability and odds
convey the exact same information. But as \(P(X)\) goes from 0 to 1,
OR(X) goes from 0 to infinity

Input values (x) are combined linearly using weights or coefficient
values (referred to as the Greek capital letter Beta) to predict an
output value (y). A key difference from linear regression is that the
output value being modeled is a binary values (0 or 1) (discrete) rather
than a numeric value (continuous)

\begin{figure}
\centering
\includegraphics{https://image.slidesharecdn.com/ihcclogisticregression-130728061733-phpapp02/95/logistic-regression-in-casecontrol-study-14-638.jpg?cb=1374992365}
\caption{alt text}
\end{figure}

However, we are still not quite there yet, since our boundary function
gives a value from --infinity to infinity. So what we do, is take the
logarithm of OR(X), called the log-odds function. Mathematically, as
OR(X) goes from 0 to infinity, log(OR(X)) goes from --infinity to
infinity

We are modeling the probability that an input (X) belongs to the default
class (Y=1). The probability prediction must be transformed into a
binary values (0 or 1) in order to actually make a probability
prediction. Logistic regression is a linear method, but the predictions
are transformed using the logistic function. The impact of this is that
we can no longer understand the predictions as a linear combination of
the inputs as we can with linear regression.

Wait, how is the boundary function computed? Well we want to maximize
the likelihood that a random data point gets classified correctly. We
call this Maximimum likelihood estimation.

Maximum likelihood estimation is a general approach to estimating
parameters in statistical models by maximizing the likelihood function.
MLE applied to deep networks gets a special name ``Backpropagation". MLE
is defined as

L(θ\textbar{}X)=f(X\textbar{}θ)

Newton's Method is an optimization algorithm. You can use this algorithm
to find maximum (or minimum) of many different functions, including the
likelihood function. You can obtain maximum likelihood estimates using
different methods and using an optimization algorithm is one of them.

\subsection{Why use Newton's Method for
optimizing?}\label{why-use-newtons-method-for-optimizing}

\begin{itemize}
\tightlist
\item
  Newton's method usually converges faster than gradient descent when
  maximizing logistic regression log likelihood.
\item
  Each iteration is more expensive than gradient descent because of
  calculating inverse of Hessian
\item
  As long as data points are not very large, Newton's methods are
  preferred
\end{itemize}

\subsection{What are some other good examples of logistic regression +
newton's
method?}\label{what-are-some-other-good-examples-of-logistic-regression-newtons-method}

1 https://github.com/hhl60492/newton\_logistic/blob/master/main.py (Spam
Classification) 2
https://github.com/yangarbiter/logistic\_regression\_newton-cg (Click
Through Rate Classification)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        \PY{c+c1}{\PYZsh{}matrix math}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{c+c1}{\PYZsh{}data manipulation}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{c+c1}{\PYZsh{}matrix data structure}
        \PY{k+kn}{from} \PY{n+nn}{patsy} \PY{k}{import} \PY{n}{dmatrices}
        \PY{c+c1}{\PYZsh{}for error logging}
        \PY{k+kn}{import} \PY{n+nn}{warnings}
\end{Verbatim}


     \#\# Setup

\subsubsection{Parameter / Data Setup}\label{parameter-data-setup}

In the below cells, there are various parameters and options to play
with involving data creation, algorithm settings, and what model you
want to try and fit.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{}outputs probability between 0 and 1, used to help define our logistic regression curve}
        \PY{k}{def} \PY{n+nf}{sigmoid}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Sigmoid function of x.\PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{k}{return} \PY{l+m+mi}{1}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{figure}
\centering
\includegraphics{http://i.imgur.com/TfPVnME.png}
\caption{alt text}
\end{figure}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{}makes the random numbers predictable}
         \PY{c+c1}{\PYZsh{}(pseudo\PYZhy{})random numbers work by starting with a number (the seed), }
         \PY{c+c1}{\PYZsh{}multiplying it by a large number, then taking modulo of that product. }
         \PY{c+c1}{\PYZsh{}The resulting number is then used as the seed to generate the next \PYZdq{}random\PYZdq{} number. }
         \PY{c+c1}{\PYZsh{}When you set the seed (every time), it does the same thing every time, giving you the same numbers.}
         \PY{c+c1}{\PYZsh{}good for reproducing results for debugging}
         
         
         \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{} set the seed}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}Step 1 \PYZhy{} Define model parameters (hyperparameters)}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} algorithm settings}
         \PY{c+c1}{\PYZsh{}the minimum threshold for the difference between the predicted output and the actual output}
         \PY{c+c1}{\PYZsh{}this tells our model when to stop learning, when our prediction capability is good enough}
         \PY{n}{tol}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8} \PY{c+c1}{\PYZsh{} convergence tolerance}
         
         \PY{n}{lam} \PY{o}{=} \PY{k+kc}{None} \PY{c+c1}{\PYZsh{} l2\PYZhy{}regularization}
         \PY{c+c1}{\PYZsh{}how long to train for?}
         \PY{n}{max\PYZus{}iter} \PY{o}{=} \PY{l+m+mi}{20} \PY{c+c1}{\PYZsh{} maximum allowed iterations}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} data creation settings}
         \PY{c+c1}{\PYZsh{}Covariance measures how two variables move together. }
         \PY{c+c1}{\PYZsh{}It measures whether the two move in the same direction (a positive covariance) }
         \PY{c+c1}{\PYZsh{}or in opposite directions (a negative covariance). }
         \PY{n}{r} \PY{o}{=} \PY{l+m+mf}{0.95} \PY{c+c1}{\PYZsh{} covariance between x and z}
         \PY{n}{n} \PY{o}{=} \PY{l+m+mi}{1000} \PY{c+c1}{\PYZsh{} number of observations (size of dataset to generate) }
         \PY{n}{sigma} \PY{o}{=} \PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{} variance of noise \PYZhy{} how spread out is the data?}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} model settings}
         \PY{n}{beta\PYZus{}x}\PY{p}{,} \PY{n}{beta\PYZus{}z}\PY{p}{,} \PY{n}{beta\PYZus{}v} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{4}\PY{p}{,} \PY{o}{.}\PY{l+m+mi}{9}\PY{p}{,} \PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{} true beta coefficients}
         \PY{n}{var\PYZus{}x}\PY{p}{,} \PY{n}{var\PYZus{}z}\PY{p}{,} \PY{n}{var\PYZus{}v} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4} \PY{c+c1}{\PYZsh{} variances of inputs}
         
         \PY{c+c1}{\PYZsh{}\PYZsh{} the model specification you want to fit}
         \PY{n}{formula} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y \PYZti{} x + z + v + np.exp(x) + I(v**2 + z)}\PY{l+s+s1}{\PYZsq{}}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} Step 2 \PYZhy{} Generate and organize our data}
         
         \PY{c+c1}{\PYZsh{}The multivariate normal, multinormal or Gaussian distribution is a generalization of the one\PYZhy{}dimensional normal }
         \PY{c+c1}{\PYZsh{}distribution to higher dimensions. Such a distribution is specified by its mean and covariance matrix.}
         \PY{c+c1}{\PYZsh{}so we generate values input values \PYZhy{} (x, v, z) using normal distributions}
         
         \PY{c+c1}{\PYZsh{}A probability distribution is a function that provides us the probabilities of all }
         \PY{c+c1}{\PYZsh{}possible outcomes of a stochastic process. }
         
         \PY{c+c1}{\PYZsh{}lets keep x and z closely related (height and weight)}
         \PY{n}{x}\PY{p}{,} \PY{n}{z} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{multivariate\PYZus{}normal}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{p}{[}\PY{n}{var\PYZus{}x}\PY{p}{,}\PY{n}{r}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{n}{r}\PY{p}{,}\PY{n}{var\PYZus{}z}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{n}\PY{p}{)}\PY{o}{.}\PY{n}{T}
         \PY{c+c1}{\PYZsh{}blood presure}
         \PY{n}{v} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{var\PYZus{}v}\PY{p}{,}\PY{n}{n}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
         
         \PY{c+c1}{\PYZsh{}create a pandas dataframe (easily parseable object for manipulation)}
         \PY{n}{A} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{n}{x}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{z}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{n}{z}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{v}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{n}{v}\PY{p}{\PYZcb{}}\PY{p}{)}
         \PY{c+c1}{\PYZsh{}compute the log odds for our 3 independent variables}
         \PY{c+c1}{\PYZsh{}using the sigmoid function }
         \PY{n}{A}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{log\PYZus{}odds}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{sigmoid}\PY{p}{(}\PY{n}{A}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{z}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{v}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{p}{[}\PY{n}{beta\PYZus{}x}\PY{p}{,}\PY{n}{beta\PYZus{}z}\PY{p}{,}\PY{n}{beta\PYZus{}v}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{n}{sigma}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{normal}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{n}\PY{p}{)}\PY{p}{)}
         
         
         
         \PY{c+c1}{\PYZsh{}compute the probability sample from binomial distribution}
         \PY{c+c1}{\PYZsh{}A binomial random variable is the number of successes x has in n repeated trials of a binomial experiment. }
         \PY{c+c1}{\PYZsh{}The probability distribution of a binomial random variable is called a binomial distribution. }
         \PY{n}{A}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{binomial}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{p}\PY{p}{)} \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{n}{A}\PY{o}{.}\PY{n}{log\PYZus{}odds}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{}create a dataframe that encompasses our input data, model formula, and outputs}
         \PY{n}{y}\PY{p}{,} \PY{n}{X} \PY{o}{=} \PY{n}{dmatrices}\PY{p}{(}\PY{n}{formula}\PY{p}{,} \PY{n}{A}\PY{p}{,} \PY{n}{return\PYZus{}type}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataframe}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}print it}
         \PY{n}{X}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/Users/sraval/anaconda/lib/python3.6/site-packages/ipykernel/\_\_main\_\_.py:3: RuntimeWarning: overflow encountered in exp
  app.launch\_new\_instance()

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}18}]:}     Intercept         x         z           v  np.exp(x)  I(v ** 2 + z)
         0         1.0  1.309888  0.988232    5.137890   3.705758      27.386150
         1         1.0 -0.300988 -0.184444   38.764534   0.740087    1502.504681
         2         1.0 -0.920497 -0.874842   11.798098   0.398321     138.320272
         3         1.0  0.174965  0.161452    0.001030   1.191205       0.161453
         4         1.0  0.755467  0.778161    1.347385   2.128606       2.593607
         5         1.0  0.555271  0.844654   97.468804   1.742413    9501.012390
         6         1.0  0.735170  0.443148 -139.913964   2.085836   19576.360461
         7         1.0 -0.168627 -0.156636   32.123394   0.844824    1031.755826
         8         1.0  0.238011 -0.166830  -56.340269   1.268723    3174.059121
         9         1.0  1.522206  1.153393   71.220417   4.582321    5073.501239
         10        1.0 -0.338652 -0.347083  102.569929   0.712730   10520.243283
         11        1.0 -0.437052 -0.227711  -13.617018   0.645938     185.195455
         12        1.0 -0.794423 -0.704078 -391.418061   0.451842  153207.394470
         13        1.0  0.853165  0.731489 -104.585247   2.347063   10938.805299
         14        1.0  1.510078  1.613199  -66.692121   4.527085    4449.452178
         15        1.0 -0.584762 -0.627751   -0.001936   0.557238      -0.627748
         16        1.0  1.258404  1.221651   -1.956957   3.519800       5.051331
         17        1.0  2.462204  2.351425   11.471611  11.730636     133.949284
         18        1.0 -0.028601  0.347158  384.361644   0.971805  147734.220458
         19        1.0 -1.266787 -1.365889 -265.060904   0.281735   70255.916701
         20        1.0 -0.495442 -0.319084  281.819388   0.609301   79421.848314
         21        1.0 -0.160855  0.758244    0.684946   0.851415       1.227395
         22        1.0 -0.332411 -0.097091  -15.476945   0.717193     239.438730
         23        1.0 -0.425501 -0.693561    1.761897   0.653442       2.410719
         24        1.0 -1.539670 -1.885771   37.932199   0.214452    1436.965981
         25        1.0 -1.132232 -1.208478    0.001036   0.322313      -1.208477
         26        1.0 -0.171040  0.075793  111.370935   0.842788   12403.560919
         27        1.0 -2.172880 -2.624163    8.536982   0.113849      70.255898
         28        1.0  1.166936  0.976308    0.079060   3.212137       0.982559
         29        1.0 -0.410733 -0.567065   -0.009517   0.663164      -0.566974
         ..        {\ldots}       {\ldots}       {\ldots}         {\ldots}        {\ldots}            {\ldots}
         70        1.0 -1.539835 -1.143572 -247.689819   0.214416   61349.102706
         71        1.0 -1.632153 -1.492005  -13.226907   0.195508     173.459068
         72        1.0 -0.764530 -1.628392    0.024868   0.465553      -1.627773
         73        1.0  0.695984  0.122698  -23.304949   2.005683     543.243344
         74        1.0 -0.977293 -0.954703   -2.203118   0.376329       3.899026
         75        1.0 -0.034838  0.051344    0.020535   0.965762       0.051766
         76        1.0 -0.987639 -1.250847    4.915997   0.372455      22.916176
         77        1.0  0.369994  0.348958   -0.000105   1.447726       0.348958
         78        1.0 -0.395449 -0.274797    0.148369   0.673377      -0.252783
         79        1.0 -0.807571 -0.846933   20.337651   0.445940     412.773107
         80        1.0 -0.100300 -0.138599 -341.225302   0.904566  116434.568185
         81        1.0  0.309325  0.539759    0.432925   1.362506       0.727183
         82        1.0 -0.478389 -0.516518   -0.051512   0.619781      -0.513864
         83        1.0 -0.344510 -0.330705  454.142390   0.708567  206244.979892
         84        1.0  1.980338  2.246921    6.981049   7.245195      50.981963
         85        1.0  0.113153 -0.733672  744.695504   1.119804  554570.659717
         86        1.0 -0.165867 -0.534900  -36.110413   0.847159    1303.427004
         87        1.0  0.973260  1.262661 -154.601778   2.646558   23902.972511
         88        1.0  0.716418  0.800440 -152.660552   2.047088   23306.044575
         89        1.0  0.461598  0.510788  -12.789236   1.586608     164.075341
         90        1.0  0.919909  0.723214  -88.942402   2.509062    7911.474052
         91        1.0 -0.754402 -1.052098   -3.346875   0.470292      10.149475
         92        1.0 -0.667074 -0.762721   -1.236098   0.513208       0.765217
         93        1.0  0.329666  0.052049    0.732793   1.390504       0.589034
         94        1.0 -2.294987 -2.477418  -50.803419   0.100763    2578.509943
         95        1.0 -1.577144 -1.593487   -8.868699   0.206564      77.060332
         96        1.0 -0.598048 -0.603793 -157.242463   0.549884   24724.588319
         97        1.0 -0.453198 -0.345774 -139.334162   0.635592   19413.662869
         98        1.0 -0.116760 -0.269528   -0.213530   0.889799      -0.223933
         99        1.0 -0.643911 -0.658863    1.694710   0.525234       2.213181
         
         [100 rows x 6 columns]
\end{Verbatim}
            
    \subsubsection{Algorithm Setup}\label{algorithm-setup}

We begin with a quick function for catching singular matrix errors that
we will use to decorate our Newton steps.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{}like dividing by zero (Wtff omgggggg universe collapses)}
        \PY{k}{def} \PY{n+nf}{catch\PYZus{}singularity}\PY{p}{(}\PY{n}{f}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Silences LinAlg Errors and throws a warning instead.\PYZsq{}\PYZsq{}\PYZsq{}}
            
            \PY{k}{def} \PY{n+nf}{silencer}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}\PY{p}{:}
                \PY{k}{try}\PY{p}{:}
                    \PY{k}{return} \PY{n}{f}\PY{p}{(}\PY{o}{*}\PY{n}{args}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}
                \PY{k}{except} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{LinAlgError}\PY{p}{:}
                    \PY{n}{warnings}\PY{o}{.}\PY{n}{warn}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Algorithm terminated \PYZhy{} singular Hessian!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                    \PY{k}{return} \PY{n}{args}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{k}{return} \PY{n}{silencer}
\end{Verbatim}


    \subsubsection{Explanation of a Single Newton
Step}\label{explanation-of-a-single-newton-step}

Recall that Newton's method for maximizing / minimizing a given function
\(f(\beta)\) iteratively computes the following estimate:

\[
\beta^+ = \beta - Hf(\beta)^{-1}\nabla f(\beta)
\]

The Hessian of the log-likelihood for logistic regression is given by:

hessian of our function = negative tranpose of (N times (p+1) times (N x
N diagional matrix of weights, each is p*(1-p) times X again

\[
Hf(\beta) = -X^TWX
\] and the gradient is:

gradient of our function = tranpose of X times (column vector - N vector
of probabilities)

\[
\nabla f(\beta) = X^T(y-p)
\] where \[
W := \text{diag}\left(p(1-p)\right)
\] and \(p\) are the predicted probabilites computed at the current
value of \(\beta\).

\subsubsection{Connection to Iteratively Reweighted Least Squares
(IRLS)}\label{connection-to-iteratively-reweighted-least-squares-irls}

\emph{For logistic regression, this step is actually equivalent to
computing a weighted least squares estimator at each iteration!} The
method of least squares is about estimating parameters by minimizing the
squared discrepancies between observed data, on the one hand, and their
expected values on the other

I.e., \[
\beta^+ = \arg\min_\beta (z-X\beta)^TW(z-X\beta)
\] with \(W\) as before and the \emph{adjusted response} \(z\) is given
by \[
z := X\beta + W^{-1}(y-p)
\]

\textbf{Takeaway:} This is fun, but in fact it can be leveraged to
derive asymptotics and inferential statistics about the computed MLE
\(\beta^*\)!

\subsubsection{Our implementations}\label{our-implementations}

Below we implement a single step of Newton's method, and we compute
\(Hf(\beta)^{-1}\nabla f(\beta)\) using \texttt{np.linalg.lstsq(A,b)} to
solve the equation \(Ax = b\). Note that this does not require us to
compute the actual inverse of the Hessian.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n+nd}{@catch\PYZus{}singularity}
        \PY{k}{def} \PY{n+nf}{newton\PYZus{}step}\PY{p}{(}\PY{n}{curr}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{lam}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}One naive step of Newton\PYZsq{}s Method\PYZsq{}\PYZsq{}\PYZsq{}}
            
            \PY{c+c1}{\PYZsh{}how to compute inverse? http://www.mathwarehouse.com/algebra/matrix/images/square\PYZhy{}matrix/inverse\PYZhy{}matrix.gif}
            
            \PY{c+c1}{\PYZsh{}\PYZsh{} compute necessary objects}
            \PY{c+c1}{\PYZsh{}create probability matrix, miniminum 2 dimensions, tranpose (flip it)}
            \PY{n}{p} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{curr}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{ndmin}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{T}
            \PY{c+c1}{\PYZsh{}create weight matrix from it}
            \PY{n}{W} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{p}{(}\PY{n}{p}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{p}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            \PY{c+c1}{\PYZsh{}derive the hessian }
            \PY{n}{hessian} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{)}
            \PY{c+c1}{\PYZsh{}derive the gradient}
            \PY{n}{grad} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{p}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{}\PYZsh{} regularization step (avoiding overfitting)}
            \PY{k}{if} \PY{n}{lam}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} Return the least\PYZhy{}squares solution to a linear matrix equation}
                \PY{n}{step}\PY{p}{,} \PY{o}{*}\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{lstsq}\PY{p}{(}\PY{n}{hessian} \PY{o}{+} \PY{n}{lam}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{n}{curr}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{grad}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{step}\PY{p}{,} \PY{o}{*}\PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{lstsq}\PY{p}{(}\PY{n}{hessian}\PY{p}{,} \PY{n}{grad}\PY{p}{)}
                
            \PY{c+c1}{\PYZsh{}\PYZsh{} update our }
            \PY{n}{beta} \PY{o}{=} \PY{n}{curr} \PY{o}{+} \PY{n}{step}
            
            \PY{k}{return} \PY{n}{beta}
\end{Verbatim}


    Next, we implement Newton's method in a \emph{slightly} different way;
this time, at each iteration, we actually compute the full inverse of
the Hessian using \texttt{np.linalg.inv()}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n+nd}{@catch\PYZus{}singularity}
        \PY{k}{def} \PY{n+nf}{alt\PYZus{}newton\PYZus{}step}\PY{p}{(}\PY{n}{curr}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{lam}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}One naive step of Newton\PYZsq{}s Method\PYZsq{}\PYZsq{}\PYZsq{}}
            
            \PY{c+c1}{\PYZsh{}\PYZsh{} compute necessary objects}
            \PY{n}{p} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{sigmoid}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{curr}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{ndmin}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{T}
            \PY{n}{W} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{p}{(}\PY{n}{p}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{p}\PY{p}{)}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
            \PY{n}{hessian} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{W}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{)}
            \PY{n}{grad} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{p}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{}\PYZsh{} regularization}
            \PY{k}{if} \PY{n}{lam}\PY{p}{:}
                \PY{c+c1}{\PYZsh{}Compute the inverse of a matrix.}
                \PY{n}{step} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{hessian} \PY{o}{+} \PY{n}{lam}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{n}{curr}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{grad}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{step} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{hessian}\PY{p}{)}\PY{p}{,} \PY{n}{grad}\PY{p}{)}
                
            \PY{c+c1}{\PYZsh{}\PYZsh{} update our weights}
            \PY{n}{beta} \PY{o}{=} \PY{n}{curr} \PY{o}{+} \PY{n}{step}
            
            \PY{k}{return} \PY{n}{beta}
\end{Verbatim}


    \subsubsection{Convergence Setup}\label{convergence-setup}

First we implement coefficient convergence; this stopping criterion
results in convergence whenever \[
\|\beta^+ - \beta\|_\infty < \epsilon
\] where \(\epsilon\) is a given tolerance.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k}{def} \PY{n+nf}{check\PYZus{}coefs\PYZus{}convergence}\PY{p}{(}\PY{n}{beta\PYZus{}old}\PY{p}{,} \PY{n}{beta\PYZus{}new}\PY{p}{,} \PY{n}{tol}\PY{p}{,} \PY{n}{iters}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}Checks whether the coefficients have converged in the l\PYZhy{}infinity norm.}
        \PY{l+s+sd}{    Returns True if they have converged, False otherwise.\PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{c+c1}{\PYZsh{}calculate the change in the coefficients}
            \PY{n}{coef\PYZus{}change} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{beta\PYZus{}old} \PY{o}{\PYZhy{}} \PY{n}{beta\PYZus{}new}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{}if change hasn\PYZsq{}t reached the threshold and we have more iterations to go, keep training}
            \PY{k}{return} \PY{o+ow}{not} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{any}\PY{p}{(}\PY{n}{coef\PYZus{}change}\PY{o}{\PYZgt{}}\PY{n}{tol}\PY{p}{)} \PY{o}{\PYZam{}} \PY{p}{(}\PY{n}{iters} \PY{o}{\PYZlt{}} \PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \subsection{Numerical Examples}\label{numerical-examples}

\subsubsection{Standard Newton with Coefficient
Convergence}\label{standard-newton-with-coefficient-convergence}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{}\PYZsh{} initial conditions}
         \PY{c+c1}{\PYZsh{}initial coefficients (weight values), 2 copies, we\PYZsq{}ll update one}
         \PY{n}{beta\PYZus{}old}\PY{p}{,} \PY{n}{beta} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}num iterations we\PYZsq{}ve done so far}
         \PY{n}{iter\PYZus{}count} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{c+c1}{\PYZsh{}have we reached convergence?}
         \PY{n}{coefs\PYZus{}converged} \PY{o}{=} \PY{k+kc}{False}
         
         \PY{c+c1}{\PYZsh{}if we haven\PYZsq{}t reached convergence... (training step)}
         \PY{k}{while} \PY{o+ow}{not} \PY{n}{coefs\PYZus{}converged}\PY{p}{:}
             
             \PY{c+c1}{\PYZsh{}set the old coefficients to our current}
             \PY{n}{beta\PYZus{}old} \PY{o}{=} \PY{n}{beta}
             \PY{c+c1}{\PYZsh{}perform a single step of newton\PYZsq{}s optimization on our data, set our updated beta values}
             \PY{n}{beta} \PY{o}{=} \PY{n}{newton\PYZus{}step}\PY{p}{(}\PY{n}{beta}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{lam}\PY{o}{=}\PY{n}{lam}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}increment the number of iterations}
             \PY{n}{iter\PYZus{}count} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
             
             \PY{c+c1}{\PYZsh{}check for convergence between our old and new beta values}
             \PY{n}{coefs\PYZus{}converged} \PY{o}{=} \PY{n}{check\PYZus{}coefs\PYZus{}convergence}\PY{p}{(}\PY{n}{beta\PYZus{}old}\PY{p}{,} \PY{n}{beta}\PY{p}{,} \PY{n}{tol}\PY{p}{,} \PY{n}{iter\PYZus{}count}\PY{p}{)}
             
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iterations : }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{iter\PYZus{}count}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Beta : }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{beta}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Iterations : 11
Beta : [[  1.79279050e+17]
 [ -1.08284603e+18]
 [ -8.55860493e+17]
 [ -1.75734896e+18]
 [ -9.70028288e+17]
 [  5.18119068e+17]]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/Users/sraval/anaconda/lib/python3.6/site-packages/ipykernel/\_\_main\_\_.py:3: RuntimeWarning: overflow encountered in exp
  app.launch\_new\_instance()

    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
